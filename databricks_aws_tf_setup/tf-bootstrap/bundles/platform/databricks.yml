bundle:
  name: platform

workspace:
  root_path: /Shared/bundles/platform
  file_path: ./

targets:
  dev:
    default: true
  prod: {}

resources:
  # Secret scope for app secrets
  secret_scopes:
    platform_secrets:
      name: platform-secrets

  # SQL warehouse
  sql_warehouses:
    wh_small:
      name: wh_small
      cluster_size: "Small"
      max_num_clusters: 1
      auto_stop_mins: 10
      enable_serverless_compute: true

  # Unity Catalog schema & volume
  schemas:
    main_silver:
      name: silver
      catalog_name: main

  volumes:
    main_silver_files:
      name: files
      catalog_name: main
      schema_name: silver
      volume_type: MANAGED

  # Job
  jobs:
    demo_job:
      name: demo_job
      tasks:
        - task_key: t1
          new_cluster:
            spark_version: 13.3.x-scala2.12
            node_type_id: i3.xlarge
            num_workers: 1
          notebook_task:
            notebook_path: ./resources/notebooks/hello.py

  # DLT pipeline (Lakeflow)
  pipelines:
    demo_pipeline:
      name: demo_pipeline
      clusters:
        - num_workers: 1
          label: "default"
      libraries: []
      configuration: {}

  # Model serving endpoint
  model_serving_endpoints:
    hello_endpoint:
      name: hello-endpoint
      config:
        served_entities:
          - name: hello
            entity_name: main.silver.hello_model
            entity_version: "1"
            workload_size: "Small"

  # Registered model (Unity Catalog)
  registered_models:
    hello_model:
      catalog_name: main
      schema_name: silver
      name: hello_model
      # model files would be provided by MLflow runs in practice

  # Dashboard (AI/BI)
  dashboards:
    platform_dashboard:
      display_name: "Platform Health"
      serialized_dashboard: |
        {"version":"1","title":"Platform Health","tiles":[]}
